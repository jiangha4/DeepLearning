import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

from copy import deepcopy
from torch.utils.data import Dataset
from torchvision import datasets, models, transforms


class CustomDataset(Dataset):
    def __init__(self, data, targets):
        self.data = data
        self.targets = targets

    def __getitem__(self, index):
        x = self.data[index]
        y = self.targets[index]

        return x, y

    def __len__(self):
        return len(self.data)


def get_custom_data(size, batch_size):
    img_index = 0
    label_index = 1

    counter = np.zeros(10)
    customDataset = CustomDataset([], [])
    customTestDataset = CustomDataset([], [])

    trainset, testset = get_data(batch_size)

    for i in range(0, 5000):
        image = testset[i][img_index]
        label = testset[i][label_index]

        customTestDataset.data.append(deepcopy(image))
        customTestDataset.targets.append(deepcopy(label))

    for data in trainset:
        image = data[img_index]
        label = data[label_index]

        if counter[label] <= size:
            customDataset.data.append(deepcopy(image))
            customDataset.targets.append(deepcopy(label))
            counter[label] += 1

        if sum(counter) >= size*10:
            break

    trainloader = torch.utils.data.DataLoader(customDataset, batch_size=batch_size, shuffle=True)
    testloader = torch.utils.data.DataLoader(customTestDataset, batch_size=batch_size, shuffle=True)

    return trainloader, testloader

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated
    plt.show()

def get_data(batch_size):
    # Data augmentation and normalization for training
    # Just normalization for validation
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(0.5, 0.5)
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(0.5, 0.5)
        ]),
    }

    trainset = datasets.MNIST('./data', download=True, train=True, transform=data_transforms['train'])
    testset = datasets.MNIST('./data', download=True, train=False, transform=data_transforms['val'])

    return trainset, testset

def train(dataloader, model, loss_fn, optimizer):
    model.train()
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

    return loss.item()


def test(dataloader, model):
    model.eval()
    size = len(dataloader.dataset)
    correct = 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            #test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    #test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100 * correct):>0.1f}%")
    return correct


def visualize_data(dataloader):
    # Get a batch of training data
    inputs, classes = next(iter(dataloader))

    # Make a grid from batch
    out = torchvision.utils.make_grid(inputs)

    imshow(out)

def create_plot(x_axis, errors, title, ylabel):
    i = 0
    for error in errors:
        i += 10
        #graph_name =
        plt.plot(x_axis, error) # label=graph_name)

    plt.xlabel("Epochs")
    plt.ylabel(ylabel)
    plt.title(title)
    plt.legend()
    plt.show()

def create_pretrained_resnet():
    model = models.resnet50(pretrained=True)
    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, 10)

    # freeze layers
    layers = 0
    for child in model.children():
        layers += 1
        if layers < 9:
            for param in child.parameters():
                param.requires_grad = False

    return model

def create_pretrained_vgg():
    model = models.vgg19(pretrained=True)
    model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    num_ftrs = model.classifier[6].in_features
    model.classifier[6] = nn.Linear(num_ftrs, 10)

    # freeze layers
    for child in model.features:
        for param in child.parameters():
            param.requires_grad = False

    layers = 0
    for child in model.classifier:
        layers += 1
        if layers < 6:
            for param in child.parameters():
                param.requires_grad = False

    return model

def main():
    epochs = 50
    x_axis = np.arange(1, epochs + 1, step=1)
    sizes = [90] #, 20, 30, 40, 50, 60, 70, 80, 90]
    batch_size = 100
    train_error_record = []
    accuracy_record = []

    for size in sizes:
        model = create_pretrained_resnet()
        # model = create_pretrained_vgg()
        criterion = nn.CrossEntropyLoss()
        optimizer_ft = optim.SGD(model.parameters(), lr=0.01)

        train_errors = []
        test_accuracy = []

        trainloader, testloader = get_custom_data(size, batch_size)
        for epoch in range(0, epochs):
            print("Epoch %s:" % epoch)
            train_loss = train(trainloader, model, criterion, optimizer_ft)
            accuracy = test(testloader, model)
            test_accuracy.append(accuracy)
            train_errors.append(train_loss)

        print(test_accuracy)
        print(train_errors)

    #     accuracy_record.append(test_accuracy)
    #     train_error_record.append(train_errors)
    #
    #
    # print(train_error_record)
    # print(accuracy_record)
    #create_plot(x_axis, train_error_record, "Train Error vs Epochs for ResNet50", "Train Error")
    #create_plot(x_axis, accuracy_record, "Accuracy vs Epoch for ResNet50", "Accuracy")

def create_graph():
    epochs = 30
    x_axis = np.arange(1, epochs + 1, step=1)

    accuracy_record_resnet = [[0.0445, 0.1095, 0.125, 0.121, 0.1255, 0.13, 0.1395, 0.1565, 0.1805, 0.178, 0.17, 0.164, 0.171, 0.17, 0.1725, 0.182, 0.193, 0.204, 0.2115, 0.2175, 0.2175, 0.217, 0.218, 0.2205, 0.228],
                          [0.0885, 0.1, 0.098, 0.0875, 0.1005, 0.122, 0.135, 0.1785, 0.2145, 0.229, 0.2225, 0.2155, 0.2405, 0.2575, 0.264, 0.269, 0.2835, 0.2745, 0.2875, 0.3035, 0.328, 0.368, 0.431, 0.45, 0.4365],
                              [0.097, 0.108, 0.1095, 0.1455, 0.1705, 0.1755, 0.225, 0.2795, 0.276, 0.2845, 0.268, 0.337,
                               0.346, 0.38, 0.372, 0.411, 0.44, 0.4835, 0.4425, 0.4555, 0.459, 0.465, 0.478, 0.459,
                               0.4965],
                          [0.0765, 0.082, 0.104, 0.153, 0.125, 0.2335, 0.2535, 0.299, 0.3665, 0.341, 0.3765, 0.46, 0.416, 0.468, 0.4865, 0.4815, 0.491, 0.509, 0.528, 0.505, 0.4845, 0.5235, 0.521, 0.508, 0.5125],
                          [0.173, 0.124, 0.122, 0.0995, 0.1315, 0.0995, 0.1655, 0.1885, 0.2745, 0.3735, 0.4455, 0.442, 0.441, 0.497, 0.495, 0.5215, 0.53, 0.5395, 0.54, 0.5545, 0.5555, 0.5645, 0.538, 0.545, 0.546],
                          [0.1345, 0.103, 0.1105, 0.0855, 0.1435, 0.136, 0.1955, 0.2685, 0.3255, 0.3675, 0.4945, 0.537, 0.5285, 0.5705, 0.5505, 0.572, 0.5925, 0.591, 0.508, 0.57, 0.593, 0.5895, 0.5865, 0.5875, 0.611],
                          [0.108, 0.105, 0.1035, 0.1775, 0.1345, 0.235, 0.2735, 0.3425, 0.384, 0.4385, 0.4655, 0.52, 0.5265, 0.5585, 0.572, 0.585, 0.571, 0.6285, 0.601, 0.601, 0.606, 0.596, 0.598, 0.616, 0.62],
                          [0.11, 0.1345, 0.1455, 0.218, 0.304, 0.3825, 0.3775, 0.521, 0.5675, 0.5355, 0.5775, 0.6515, 0.5975, 0.5945, 0.645, 0.6195, 0.6705, 0.6515, 0.664, 0.654, 0.6355, 0.6745, 0.634, 0.655, 0.677],
                          [0.081, 0.103, 0.09, 0.1815, 0.3145, 0.3875, 0.5075, 0.547, 0.53, 0.557, 0.5345, 0.5435, 0.581, 0.6105, 0.61, 0.6065, 0.615, 0.638, 0.6095, 0.637, 0.622, 0.6305, 0.642, 0.6215, 0.644]]

    train_error_record_resnet = [[2.3282620906829834, 2.2871248722076416, 2.2576351165771484, 2.2325170040130615, 2.209167242050171, 2.186647891998291, 2.1646196842193604, 2.142956018447876, 2.1216063499450684, 2.10054874420166, 2.0797739028930664, 2.0592737197875977, 2.039045572280884, 2.01908540725708, 1.9993903636932373, 1.9799580574035645, 1.9607850313186646, 1.9418693780899048, 1.923207402229309, 1.904797077178955, 1.886635184288025, 1.8687189817428589, 1.8510453701019287, 1.8336116075515747, 1.8164145946502686],
                       [2.274327278137207, 2.30674147605896, 2.2187116146087646, 2.21125864982605, 2.1704142093658447, 2.1836886405944824, 2.1288981437683105, 2.085385322570801, 2.092996835708618, 2.059248924255371, 2.0396928787231445, 2.059262990951538, 2.0350148677825928, 2.006744146347046, 1.9601163864135742, 1.9011191129684448, 1.9177484512329102, 1.9327764511108398, 1.897188425064087, 1.8772257566452026, 1.850297451019287, 1.806989312171936, 1.8235821723937988, 1.7941888570785522, 1.8178777694702148],
                       [2.290302038192749, 2.3643288612365723, 2.2544898986816406, 2.2132411003112793, 2.1712915897369385, 2.150782823562622, 2.1121437549591064, 2.1490259170532227, 2.060020923614502, 2.0453286170959473, 2.0915355682373047, 2.0202572345733643, 1.9473521709442139, 1.9649369716644287, 1.9482916593551636, 1.898135781288147, 1.873823642730713, 1.924704909324646, 1.8778936862945557, 1.8834185600280762, 1.848313331604004, 1.8116960525512695, 1.8208409547805786, 1.7457687854766846, 1.7745219469070435],
                       [2.3149282932281494, 2.2676706314086914, 2.2469327449798584, 2.219367742538452, 2.1579043865203857, 2.1141772270202637, 2.1566126346588135, 2.0768206119537354, 2.082158088684082, 2.061974048614502, 1.9710372686386108, 1.9350215196609497, 1.9336953163146973, 1.9034212827682495, 1.8618541955947876, 1.8481855392456055, 1.8394604921340942, 1.7763280868530273, 1.8527640104293823, 1.7749923467636108, 1.7627536058425903, 1.7524038553237915, 1.7210991382598877, 1.6867201328277588, 1.7154417037963867],
                       [2.292280673980713, 2.251974582672119, 2.1997547149658203, 2.170955181121826, 2.147407293319702, 2.1498470306396484, 2.0808069705963135, 2.068181276321411, 2.00744891166687, 1.9826104640960693, 1.9524730443954468, 1.9130969047546387, 1.94380784034729, 1.8718229532241821, 1.8139290809631348, 1.7743035554885864, 1.8195148706436157, 1.7743257284164429, 1.73122239112854, 1.6672645807266235, 1.6257655620574951, 1.657499074935913, 1.695863962173462, 1.5981463193893433, 1.599787950515747],
                       [2.2821619510650635, 2.2207751274108887, 2.2101781368255615, 2.178062915802002, 2.130627393722534, 2.066068410873413, 2.0396621227264404, 1.9869381189346313, 1.979766845703125, 1.9117240905761719, 1.8999550342559814, 1.8661508560180664, 1.8301771879196167, 1.8192814588546753, 1.8289120197296143, 1.8103337287902832, 1.8205976486206055, 1.765089750289917, 1.624457597732544, 1.7319056987762451, 1.6611160039901733, 1.7077109813690186, 1.6482161283493042, 1.545009970664978, 1.5338314771652222],
                       [2.452160, 2.258570, 2.190335, 2.156968, 2.064395, 2.034470, 2.051842, 1.989250, 1.947916, 1.848298, 1.859792, 1.756027, 1.867137, 1.712488, 1.681478, 1.670443, 1.677411, 1.583928, 1.594617, 1.586311, 1.594475, 1.608597, 1.545463, 1.535649, 1.481638],
                       [2.322647, 2.254801, 2.189662, 2.148081, 2.052012, 1.996311, 2.029983, 1.916620, 1.856633, 1.830675, 1.855630, 1.717241, 1.702387, 1.656400, 1.684066, 1.602621, 1.604510, 1.581813, 1.521306, 1.544896, 1.665972, 1.546526, 1.499449, 1.441493, 1.453662],
                       [2.2733194828033447, 2.2418715953826904, 2.1858952045440674, 2.0566868782043457, 2.049334764480591, 2.0381269454956055, 1.9410772323608398, 1.8932716846466064, 1.8805636167526245, 1.8754042387008667, 1.8221943378448486, 1.781019926071167, 1.7089848518371582, 1.7460991144180298, 1.6176786422729492, 1.6624232530593872, 1.6387345790863037, 1.5722763538360596, 1.5959547758102417, 1.5972992181777954, 1.5902819633483887, 1.552026629447937, 1.4975260496139526, 1.4138329029083252, 1.5145074129104614]]

    accuracy_record_vgg = [[0.0405, 0.056, 0.1, 0.1255, 0.1635, 0.185, 0.2105, 0.2095, 0.24, 0.25, 0.25, 0.231, 0.247, 0.256, 0.2435, 0.2595, 0.269, 0.2635, 0.2595, 0.267, 0.2595, 0.2585, 0.2625, 0.2665, 0.2665],
                           [0.128, 0.1925, 0.228, 0.1045, 0.171, 0.1335, 0.2085, 0.2725, 0.2225, 0.217, 0.2545, 0.2815, 0.2365, 0.2515, 0.253, 0.2775, 0.2455, 0.2705, 0.267, 0.273, 0.3215, 0.2885, 0.283, 0.282, 0.346],
                           [0.17, 0.22, 0.3185, 0.293, 0.3735, 0.456, 0.4375, 0.4325, 0.474, 0.496, 0.5195, 0.488, 0.523, 0.5675, 0.5225, 0.573, 0.528, 0.5585, 0.5695, 0.605, 0.586, 0.592, 0.555, 0.571, 0.5865],
                           [0.141, 0.2335, 0.2675, 0.2975, 0.3425, 0.3805, 0.3905, 0.398, 0.4615, 0.502, 0.4265, 0.4955, 0.4935, 0.4965, 0.5215, 0.5245, 0.5445, 0.532, 0.561, 0.5485, 0.475, 0.5515, 0.56, 0.539, 0.55],
                           [0.1885, 0.188, 0.2045, 0.2835, 0.3545, 0.308, 0.24, 0.3985, 0.316, 0.4465, 0.4465, 0.3985, 0.4275, 0.4325, 0.358, 0.4215, 0.485, 0.45, 0.431, 0.448, 0.446, 0.4425, 0.496, 0.4985, 0.522],
                           [0.232, 0.3585, 0.3645, 0.4145, 0.3795, 0.38, 0.374, 0.3815, 0.3965, 0.4225, 0.464, 0.499, 0.488, 0.5085, 0.5105, 0.4955, 0.485, 0.507, 0.5365, 0.507, 0.493, 0.5125, 0.549, 0.5045, 0.526],
                           [0.1045, 0.2275, 0.3595, 0.272, 0.3735, 0.4125, 0.455, 0.4335, 0.3915, 0.4335, 0.3625, 0.504, 0.433, 0.5065, 0.428, 0.484, 0.5245, 0.4505, 0.489, 0.4945, 0.4485, 0.5395, 0.51, 0.4565, 0.5415],
                           [0.1545, 0.331, 0.3175, 0.386, 0.372, 0.4265, 0.419, 0.4335, 0.4495, 0.4465, 0.4685, 0.478, 0.444, 0.471, 0.4125, 0.414, 0.4605, 0.467, 0.4695, 0.465, 0.451, 0.4725, 0.4865, 0.4995, 0.5285],
                           [0.257, 0.21, 0.3225, 0.362, 0.4255, 0.3505, 0.359, 0.337, 0.3375, 0.369, 0.3765, 0.392, 0.4145, 0.4155, 0.4375, 0.4455, 0.391, 0.452, 0.429, 0.457, 0.4455, 0.425, 0.463, 0.4875, 0.455]]

    train_error_record_vgg = [[2.3287293910980225, 2.323479413986206, 2.333400011062622, 2.320321798324585, 2.318476676940918, 2.2811503410339355, 2.269406795501709, 2.284080743789673, 2.3041858673095703, 2.2621445655822754, 2.256270170211792, 2.2576656341552734, 2.242436408996582, 2.245309829711914, 2.257235050201416, 2.237612724304199, 2.2449731826782227, 2.253580093383789, 2.24798583984375, 2.2305846214294434, 2.2358651161193848, 2.22780179977417, 2.2504842281341553, 2.2320022583007812, 2.241697072982788],
                              [2.300494909286499, 2.382739305496216, 2.3278768062591553, 2.2769935131073, 2.3463377952575684, 2.3711724281311035, 2.3504798412323, 2.3007047176361084, 2.3402791023254395, 2.307046413421631, 2.311093807220459, 2.232175827026367, 2.2586302757263184, 2.2958011627197266, 2.3315587043762207, 2.3454647064208984, 2.297761917114258, 2.2111611366271973, 2.253279209136963, 2.223958969116211, 2.1260488033294678, 2.260103225708008, 2.144497871398926, 2.1223151683807373, 2.2504360675811768],
                              [2.3511481285095215, 2.295088291168213, 2.2853004932403564, 2.2152304649353027, 2.1777939796447754, 2.183471441268921, 2.1211838722229004, 2.051941394805908, 2.1193885803222656, 2.037928581237793, 2.043179988861084, 2.0526013374328613, 1.95475435256958, 1.9674488306045532, 1.9401549100875854, 1.944060206413269, 1.9139896631240845, 1.9486570358276367, 1.8346360921859741, 1.8993240594863892, 1.8467146158218384, 1.8670718669891357, 1.7941882610321045, 1.8086587190628052, 1.7662769556045532],
                              [2.3286406993865967, 2.311579942703247, 2.2547855377197266, 2.2749276161193848, 2.2357535362243652, 2.1954047679901123, 2.1890146732330322, 2.22116756439209, 2.1795167922973633, 2.1575767993927, 2.1426920890808105, 2.130128860473633, 2.0836634635925293, 2.044778823852539, 2.112398862838745, 2.0898070335388184, 2.0410349369049072, 2.0452332496643066, 2.03731632232666, 2.0545899868011475, 2.016631841659546, 2.0580148696899414, 1.9999732971191406, 2.0684821605682373, 2.083747625350952],
                              [2.2901875972747803, 2.2666585445404053, 2.324418306350708, 2.2653920650482178, 2.262498140335083, 2.221851348876953, 2.232776165008545, 2.1865570545196533, 2.1622142791748047, 2.1657660007476807, 2.1484603881835938, 2.219331741333008, 2.146965980529785, 2.1451659202575684, 2.0834174156188965, 2.0980637073516846, 2.0154526233673096, 2.0769641399383545, 2.100749969482422, 2.0452327728271484, 2.041181802749634, 2.022387742996216, 2.029681921005249, 1.9794800281524658, 1.9725449085235596],
                              [2.2480127811431885, 2.23453950881958, 2.2340619564056396, 2.192101001739502, 2.119417667388916, 2.157339572906494, 2.1705803871154785, 2.024407148361206, 2.0345711708068848, 2.019542932510376, 2.060220956802368, 2.0316693782806396, 2.024632453918457, 1.9765114784240723, 2.0018696784973145, 1.9148788452148438, 1.9687968492507935, 1.8485515117645264, 1.932808518409729, 1.9375462532043457, 1.9371532201766968, 1.9084656238555908, 1.8873578310012817, 1.9356273412704468, 1.950709581375122],
                              [2.2609901428222656, 2.26733136177063, 2.257709503173828, 2.2682924270629883, 2.209026575088501, 2.2097764015197754, 2.1861984729766846, 2.1894824504852295, 2.1773452758789062, 2.115571975708008, 2.1867117881774902, 2.118351936340332, 2.146364688873291, 2.1076912879943848, 2.1241512298583984, 2.063096523284912, 2.1229841709136963, 2.0570945739746094, 2.078411102294922, 2.059136390686035, 2.070519208908081, 2.0041418075561523, 2.0755202770233154, 1.9773077964782715, 2.06400990486145],
                              [2.303443193435669, 2.2367568016052246, 2.216000556945801, 2.1554622650146484, 2.186394214630127, 2.1547956466674805, 2.128288984298706, 2.1550543308258057, 2.0845563411712646, 2.0842597484588623, 2.0754246711730957, 2.0485243797302246, 2.036907434463501, 1.9799212217330933, 2.0076050758361816, 1.906285285949707, 1.9443570375442505, 1.9741451740264893, 1.9134002923965454, 1.9303094148635864, 1.9606518745422363, 1.9133965969085693, 1.9190673828125, 1.9796111583709717, 1.937339186668396],
                              [2.2946834564208984, 2.235623836517334, 2.2277302742004395, 2.2193078994750977, 2.2466208934783936, 2.232161283493042, 2.1917929649353027, 2.128164052963257, 2.1059927940368652, 2.1336238384246826, 2.0747315883636475, 2.1357691287994385, 2.0873656272888184, 2.0997703075408936, 2.0466670989990234, 2.1191675662994385, 2.100036382675171, 2.0685298442840576, 1.9751403331756592, 1.9618444442749023, 2.0246403217315674, 2.032947301864624, 2.10249400138855, 2.0350213050842285, 1.997260332107544]]

    accuracy_record_resnet_50 = [[0.2138, 0.1492, 0.1864, 0.258, 0.3778, 0.4696, 0.4792, 0.5536, 0.5522, 0.6002, 0.612, 0.6284, 0.6264, 0.6254, 0.607, 0.6396, 0.637, 0.6526, 0.6196, 0.6236, 0.6646, 0.6396, 0.6562, 0.6522, 0.663, 0.6668, 0.648, 0.6484, 0.6826, 0.6786, 0.6702, 0.6546, 0.667, 0.6692, 0.6836, 0.6758, 0.6842, 0.677, 0.6808, 0.6854, 0.6788, 0.6908, 0.6922, 0.6824, 0.6858, 0.682, 0.6894, 0.688, 0.693, 0.6934]]

    train_error_record_resnet_50 = [[2.245394706726074, 2.216895341873169, 2.155245304107666, 2.084354877471924, 1.9715070724487305, 2.0029451847076416, 1.9803352355957031, 1.9611648321151733, 1.8218324184417725, 1.7557415962219238, 1.7753164768218994, 1.7367411851882935, 1.6356959342956543, 1.653651237487793, 1.661832332611084, 1.6090339422225952, 1.6016725301742554, 1.597448468208313, 1.54323410987854, 1.450775146484375, 1.55915105342865, 1.5142261981964111, 1.4338477849960327, 1.4033455848693848, 1.4881423711776733, 1.3687025308609009, 1.3096923828125, 1.321630597114563, 1.4121710062026978, 1.3069627285003662, 1.4929497241973877, 1.212927222251892, 1.3719568252563477, 1.2742438316345215, 1.2726716995239258, 1.2690322399139404, 1.1336947679519653, 1.2874112129211426, 1.1854467391967773, 1.237795114517212, 1.2723244428634644, 1.309216022491455, 1.1903645992279053, 1.2671512365341187, 1.2029619216918945, 1.309023141860962, 1.1578630208969116, 1.2439305782318115, 1.2325059175491333, 1.1063432693481445]]

    accuracy_record_vgg50 = [[0.1555, 0.2545, 0.28, 0.387, 0.3715, 0.4285, 0.424, 0.51, 0.3385, 0.451, 0.5465, 0.5135, 0.556, 0.576, 0.542,
     0.579, 0.5505, 0.541, 0.5455, 0.5995, 0.5595, 0.5945, 0.5905, 0.608, 0.5855, 0.5525, 0.5535, 0.5865, 0.5635, 0.604,
     0.591, 0.599, 0.6, 0.607, 0.606, 0.6175, 0.645, 0.6225, 0.6355, 0.5885, 0.619, 0.584, 0.6255, 0.6225, 0.6265,
     0.5615, 0.6155, 0.6035, 0.624, 0.5745]]
    train_error_record_vgg50 = [[2.3125338554382324, 2.2825725078582764, 2.2580296993255615, 2.1738741397857666, 2.2182273864746094,
     2.1794509887695312, 2.145699977874756, 2.158374071121216, 2.063748836517334, 2.0711944103240967,
     2.0995287895202637, 2.0797104835510254, 1.9564403295516968, 1.9946887493133545, 2.0780935287475586,
     2.040708541870117, 2.050391912460327, 2.000685214996338, 2.027494430541992, 1.9892780780792236, 1.9802887439727783,
     1.9038374423980713, 1.9168694019317627, 2.057762384414673, 2.0027084350585938, 1.978144645690918,
     1.9550243616104126, 1.8532403707504272, 1.9119056463241577, 1.9568336009979248, 1.8701246976852417,
     1.9269713163375854, 1.813462495803833, 1.804931640625, 1.8136509656906128, 1.8297321796417236, 1.9253530502319336,
     1.8024334907531738, 1.8333659172058105, 1.8759557008743286, 1.8059587478637695, 1.8584774732589722,
     1.8060723543167114, 1.75654935836792, 1.8062251806259155, 1.8429670333862305, 1.8455294370651245,
     1.7962327003479004, 1.79960036277771, 1.8276879787445068]]

    train_stn = [[1.4534554481506348, 1.543115496635437, 1.1974276304244995, 1.8610402345657349, 1.2714002132415771, 1.0558998584747314, 0.9723178148269653, 1.2370312213897705, 1.0526058673858643, 1.3050976991653442, 1.0125755071640015, 1.080429196357727, 0.84479820728302, 1.4105008840560913, 1.3662554025650024, 0.9347084760665894, 0.6822168231010437, 1.1634036302566528, 1.2001140117645264, 0.6324514746665955, 0.9507760405540466, 0.506773054599762, 1.087122917175293, 1.2898166179656982, 1.0466972589492798, 1.0652004480361938, 1.0017709732055664, 1.2085375785827637, 0.9605094790458679, 1.2738268375396729]]
    test_stn = [[1.6597316574096679, 1.6076095194498699, 1.3568713386535645, 1.6390920377095541, 1.3210654483795166, 1.0217054514567057, 1.0939600018819173, 0.9672074827829997, 1.0111282318115233, 0.9732870826721192, 0.9661331015904745, 0.9853349514007569, 0.9512771323521932, 0.9358742375691732, 0.9590949296315511, 0.9504382901509603, 0.9274694886525472, 0.9617942199707031, 0.9617942504882813, 0.9158864067077637, 0.924421842956543, 0.9305544082641601, 0.9425010420481363, 0.9130194348653158, 0.92345385055542, 0.9417188460032145, 0.9092066664377848, 0.9124963591257731, 0.9091106568654378, 0.9130782830556233]]
    acc = [[40.17333333333333, 40.01, 51.46666666666667, 36.95333333333333, 50.59, 63.196666666666665, 60.46666666666667, 64.86, 63.586666666666666, 64.55333333333333, 64.82666666666667, 64.28333333333333, 65.35666666666667, 65.59333333333333, 64.61333333333333, 65.18, 65.71333333333334, 64.86666666666666, 65.32666666666667, 66.03666666666666, 65.89666666666666, 65.75333333333333, 65.74666666666667, 66.29, 66.03, 65.45666666666666, 66.43, 66.17, 66.32, 66.16]]

    create_plot(x_axis, test_stn, "Test Error vs Epochs for STN", "Train Error")
    create_plot(x_axis, acc, "Accuracy vs Epoch for STN", "Accuracy")

if __name__ == '__main__':
    #main()
    create_graph()
